---
title: "The _rinform_ Package"
author: "Gabriele Valentini"
date: "`r Sys.Date()`"
output:
#  rmarkdown::html_vignette
  html_document:
    toc: true # table of content true
    toc_depth: 2  # upto three depths of headings (specified by #, ## and ###)
    toc_float: true    
    number_sections: true  ## if you want number sections at each table header
    theme: yeti  # many options for theme, this one is my favorite.
    highlight: tango  # specifies the syntax highlighting style
  pdf_document:
    highlight: null
    number_sections: yes    
bibliography: vignette.bibtex
vignette: >
  %\VignetteIndexEntry{The rinform Wrapper}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

An R wrapper of the [Inform v1.0.0](https://elife-asu.github.io/Inform/) C library for performing information analysis of complex system. As for the Inform library, _rinform_ is structured around the concepts of:

* discrete emperical probability distributions which form the basis for
  all of the information-theoretic measures,
* classic information-theoretic measures built upon empirical distributions,
* measures of information dynamics for time series.

In addition to the core components, rinform also provides a small collection of utilities to deal with time series. 

If you are using _rinform_, consider citing the following articles:

* D.G. Moore, G. Valentini, S.I. Walker, M. Levin. "Inform: Efficient 
Information-Theoretic Analysis of Collective Behaviors". _Frontiers in Robotics & AI.
(_under review_)
* D.G. Moore, G. Valentini, S.I. Walker, M. Levin. "Inform: A Toolkit for
Information-Theoretic Analysis of Complex Systems". In: _Proceedings of the 
2017 IEEE Symposium Series on Computational Intelligence, Symposium on 
Artificial Life_, IEEE Press, 2017. (_in press_)

# Getting Started

## Installation 

## Getting Help

*rinform*, as its parent library [inform](https://github.com/elife-asu/inform),
is developed to help anyone
interested in applying information-theoretic techniques get things done
quickly and painlessly. We cannot do that without your feedback. We host the
project's source code and issue tracker on [Github](https://github.com/elife-asu/rinform).
Please create an issue if you find a bug, an error in this documentation,
or have a feature you'd like to request. Your contribution will make
*rinform* a better tool for everyone.

If you are interested in contributing to *rinform*, please contact the developers,
and we'll get you up and running!

Resources: [Source Repository](https://github.com/elife-asu/rinform) and [Issue Tracker](https://github.com/elife-asu/rinform/issues)

## Related Projects

### Inform Community

While *rinform* is a great tool to use directly in R, significant effort has gone
into making it easy to access the functionality of the [*inform*](https://github.com/elife-asu/inform) library directly
using other higher level languages. Here are a few of the wrappers that are
under active development:

* _PyInform_ for Python ([repository](https://github.com/elife-asu/pyinform), [webpage](https://elife-asu.github.io/PyInform))
* _Inform.jl_ for Julia ([repository](https://github.com/elife-asu/inform.jl), webpage: _forthcoming_)
* _InformWolfram_ for Mathematica Wolfram Language ([repository](https://github.com/elife-asu/informwolfram), webpage: _forthcoming_)

### Intellectual Relatives

*rinform* was not created in an intellectual vacuum. Many similar projects have preceded it
and are under active development. All of those projects have advantages and disadvantages
compared to *rnform*. If *Inform* doesn't meet your needs, I hope you will let us know, and
try one of these:

* [JIDT: Java Information Dynamics Toolkit](https://jlizier.github.il/jidt) (Java)
* [TRENTOOL: TRansfer ENtropy TOOLbox](https://trentool.github.io/TRENTOOL3) (Matlab)
* [dit: discrete information theory](https://docs.dit.io) (Python)
* [MuTE: MuTE Toolbox- The Dynamic DIrected Links Detector](https://mutetoolbox.guru) (Matlab)
* [MVGC: Multivariate Granger Causality Toolbox](https://users.sussex.ac.uk/~lionelb/MVGC) (Matlab)
* [ACSS: Algorithmic Complexity for Short
    Strings](https://cran.r-project.org/web/packages/acss) (R)
* [OACC: Online Algorithmic Complexity
    Calculator](https://complexitycalculator.com) (web-based, R)

## Copyright and Licensing

Copyright © 2017-2018 ELIFE, Arizona State University. Free use of this software is granted
under the terms of the MIT License. See the [LICENSE](https://github.com/elife-asu/rinform/blob/master/LICENSE) file for details.

# Empirical Distributions {#Distributions}


The [`Dist`](#DistClass) class provides an _empirical_ distribution,
i.e. a histogram, representing the observed frequencies of some fixed-sized set of events.
This class is the basis for all of the fundamental information measures provided on
discrete probability distributions.

The purpose of the [`Dist`](#DistClass) class is to make writing custom
implementations of information-theoretic measures possible. Most of the measures implemented
in _rinform_ use this structure at their core.

+--------------------+----------------------------------------------------------+
| Type               | Functions                                                |
+====================+==========================================================+
| Class              |[`Dist`](#DistClass)                                      |
+--------------------+----------------------------------------------------------+
| Allocation         | [`resize`](#Resize), [`copy`](#Copy), [`infer`](#Infer), [`approximate`](#Approximate), [`uniform`](#Uniform) |
+--------------------+---------------------------------------------------------------+
| Accessors/Mutators | [`length`](#length), [`counts`](#Counts), [`valid`](#Valid), [`get_item`](#GetItem), [`set_item`](#SetItem), [`tick`](#Tick), [`accumulate`](#Accumulate) |
+--------------------+----------------------------------------------------------+
| Probabilities      |[`probability`](#Probability), [`dump`](#Dump)            |
+--------------------+----------------------------------------------------------+

## Distribution Class {#DistClass}

This class is the basis for almost all of the calculations performed via the library. It
provides a means of managing observations and converting the observed event frequencies to
probabilities.

The distribution is, roughly, a histogram with finite support. The events are assumed to be
mapped to the dense set of integers $\{0,1,\ldots,N-1\}$ where $N$ is the number
of observable events. The number of observable events can be extracted with
[`length`](#length).

Whenever the size of the support or the number of observed events is zero, the distribution
is considered invalid meaning that you can't trust any probabilities extracted from it.

__Interface:__

A distribution of observed event frequencies. If the parameter `n` is an integer,
the distribution is constructed with a zeroed support of size $n$. If `n` is a vector
of integer values, the sequence is treated as the underlying support. On the other hand,
if `n` is a vector of floating point values, it is treated as a probability distribution
and must sum to unity. Note, if a probability distribution is given as the underlying
support, it will first be converted to a histogram with a precision of 9 significant figures.

```r
Dist(n)
```

__Examples:__

Create an empty distribution with support size 3:
```{r}
Dist(3)
```

Create a distribution with 2 events, the first observed 5 times, the second
observed 3 times:
```{r}
Dist(c(5, 3))
```

## Allocation

### Resize {#Resize}

If the distribution is `NULL`, a new distribution of the requested size is created.

If the distribution already has the requested size, then the original distribution is
returned. If the requested size is greater than the current size, then the newly observable
events are assigned zero frequency. If the requested size is less than the current size,
then the discarded events are lost and the observation count is adjusted accordingly.

Note that the distribution cannot be reduced to size `0`. In that case the distribution is
left unmodified.

__Interface:__

Resize the distribution `d` to have new support of size `n`.

```r
resize(d, n)
```
__Examples:__

Create a distribution with size 2:
```{r}
d <- Dist(c(2, 13))
d
```

Increase the size of the support to 5:
```{r}
d <- resize(d, 5)
d
```

Decrease the size of the support to 3:
```{r}
d <- resize(d, 3)
d
```

### Copy {#Copy}

If the source distribution is `NULL`, then `NULL` is returned.

__Interface:__

Return a copy of the distribution `d`.

```r
copy(d)
```

__Examples:__

Create a base distribution `d`:
```{r}
d <- Dist(c(1:5))
d
```

Copy distribution to a different object `p`:
```{r}
p <- copy(d)
p
```

### Infer {#Infer}

__Interface:__

Infer a distribution from a collection of observed `events`.

```r
infer(events)
```

__Examples:__

Create a distribution $d = \{3/5, 2/5\}$:
```{r}
d <- infer(c(0, 0, 1, 0, 1))
dump(d)
```

Create a distribution $d = \{3/8, 3/8, 2/8\}$:
```{r}
d <- infer(c(0, 0, 1, 0, 1, 2, 2, 1))
dump(d)
```

### Approximate {#Approximate}

__Interface:__

Approximate a given probability distribution to a given tolerance.

```r
approximate(probs, tol)
```

__Examples:__

Approximate a distribution with tolerance $10^{-3}$:
```{r}
probs <- c(0.5, 0.2, 0.3)
d     <- approximate(probs, 1e-3)
d$histogram

probs <- c(1.0 / 3, 2.0 / 3)
d     <- approximate(probs, 1e-3)
d$histogram
```

### Uniform {#Uniform}

__Interface:__

Create a uniform distribution of a given size `n`. The support size `n` must
define a valid support (i.e., `n` must be greater than 0).

```r
uniform(n)
```
__Examples:__

Initialize a uniform distribution of size 3:
```{r}
d <- uniform(3)
d

dump(d)
```

## Accessors/Mutators

### Length {#length}

__Interface:__

Get the size of the distribution's support. If the distribution is `NULL`, then a support
of 0 is returned.

```r
length(d)
```

__Examples:__

```{r}
# NULL distribution
d <- NULL
length(d)

# Distribution with size 5
d <- Dist(5)
length(d)
```

### Counts {#Counts}

__Interface:__

Get the total number of observations so far made.

```
counts(d)
```

__Examples:__

```{r}
# Counts of valid distribution
dist <- Dist(c(5, 10))
counts(dist) == 15

dist <- set_item(dist, 2, 5)
counts(dist) == 10
```

### Valid {#Valid}

__Interface:__

Determine whether or not the distribution is valid. In order to safely extract probabilities,
the size of the support must be non-zero and the number of observations must be non-zero. In any
other case, the distribution is invalid.

```r
valid(d)
```

__Examples:__

```{r}
# Distribution with 0 observations
dist <- Dist(3)
valid(dist)

# Valid distribution
dist <- Dist(c(1:5))
valid(dist)

# Invalid distribution with 0 support
dist$size <- as.integer(0)
valid(dist)
```

### Get Item {#GetItem}

__Interface:__

Get the number of occurrences of a given `event`.

```r
get_item(d, event)
```

__Examples:__

```{r}
# Get item from a valid distribution
dist <- Dist(c(3, 2, 1, 0))
get_item(dist, 1) == 3
get_item(dist, 2) == 2
get_item(dist, 3) == 1
get_item(dist, 4) == 0
```

### Set Item {#SetItem}

__Interface:__

Set the number of occurrences of a given `event`. This function manually sets the number
of occurrences of a given `event`.  Note that the only
restriction is that the value be positive. This means that this function can be used to
invalidate the distribution by changing all of the event frequencies to zero.

```r
set_item(d, event, value)
```

__Examples:__

```{r}
# Initialize empty distribution
dist <- Dist(2)
dist

# Set item into a valid distribution
dist <- set_item(dist, 1, 3)
dist <- set_item(dist, 2, 8)
dist
```

### Tick {#Tick}

__Interface:__

Increment the number of observations of a given `event` and return an updated
copy of the distribution `d`. As an alternative to [`set_item`](#SetItem), this
function simply increments the number of occurrences of a given event. This
is useful when iteratively observing events.

```r
tick(d, event)
```

__Examples:__

```{r}
# Initial distribution
dist <- Dist(c(2, 4))

# Adding an observation for each event
dist <- tick(dist, 1)
get_item(dist, 1) == 3

dist <- tick(dist, 2)
get_item(dist, 2) == 5
```

### Accumulate {#Accumulate}

__Interface:__

Accumulate observations from a series. If an invalid distribution is provided, no
events will be observed and an error will be raised. If an invalid event is
provided, then the number of valid events to that point will be added and a
warning will be raised.

```r
accumulate(d, events)
```

__Examples:__

```{r}
# Create a valid distribution
d <- Dist(c(1, 2, 3))
dump(d)

# Accumulate events
events <- c(0, 0, 1, 0, 1)
d <- accumulate(d, events)
dump(d)

# Accumulate invalid events
events <- c(0, 1, 1, 3, 1)
d <- accumulate(d, events)
dump(d)
```

## Probabilities

### Probability {#Probability}

__Interface:__

Extract the probability of an `event` from a distribution `d`. This function simply computes the
probability of a given `event` and returns that value. 

```r
probability(d, event)
```

__Example:__

```{r}
# Initialize distribution
dist <- Dist(c(2, 2, 4))

# Compute probabilities
probability(dist, 1) == 0.25
probability(dist, 2) == 0.25
probability(dist, 3) == 0.50
```

### Dump {#Dump}

__Interface:__

Dump the probabilities of all events to an array. This function computes the
probabilities of all of the events in the support and return them as an array.

```r
dump(d)
```

__Example:__

```{r}
# Initialize distribution and dump probabilities
dist <- Dist(c(2, 2, 4))
dump(dist)

# Modify and dump again
dist <- set_item(dist, 1, 12)
dump(dist)
```

# Shannon Information Measures

The _rinform_ package provides a collection of entropy and other information measures
defined on discrete probability distributions (see [Dist](#Distributions)).
These functions form the core of _Inform_ as all of the time series analysis functions
are built upon them. _rinform_ provides access to these functions in the case
the user wants to use them outside the (already wrapped) time series measures.

## Entropy

Taking $X$ to be a random variable with $p_X$ a probability distribution on
$X$, the base-$b$ Shannon entropy is defined as 
$$
H(X) = - \sum_{x} p_X(x) \log_b p_X(x).
$$

See [[@Shannon1948]](#References) for more details.

__Interface:__

Compute the base-`b` Shannon entropy of the distribution `p`.
```r
shannon_entropy(p, b = 2)
```
    
__Example:__

Compute the Shannon entropy of a uniform distribution:
```{r}
p <- Dist(c(1, 1, 1, 1))
shannon_entropy(p)
shannon_entropy(p, b = 4)
```

Compute the Shannon entropy of a non-uniform distribution:
```{r}
p <- Dist(c(2, 1))
shannon_entropy(p)
shannon_entropy(p, b = 3)
```

## Mutual Information

[Mutual information](https://en.wikipedia.org/wiki/Mutual_information) provides a
measure of the mutual dependence between two random variables. Let $X$ and $Y$ be
random variables with probability distributions $p_X$ and $p_Y$ respectively, and
$p_{X,Y}$ the joint probability distribution over $(X,Y)$. The base-$b$ mutual
information between $X$ and $Y$ is defined as
$$
\begin{split}
I(X;Y) &= \sum_{x,y} p_{X,Y}(x,y) \log_b \frac{p_{X,Y}(x,y)}{p_X(x)p_Y(y)} \\
       &= H(X) + H(Y) - H(X,Y).
\end{split}
$$
Here the second line takes advantage of the properties of logarithms and the
definition of Shannon entropy.

To some degree one can think of mutual information as a measure of the
(linear and non-linear) coorelations between random variables.

See [[@Cover1991]](#References) for more details.

__Interface:__

Compute the base-`b` mutual information between two random variables $X$ and $Y$.
```r
shannon_mutual_info(p_xy, p_x, p_y, b = 2)
```

__Examples:__

Compute the base-2 mutual information between two random variables:
```{r}
xy <- Dist(c(10, 70, 15, 5))
x  <- Dist(c(80, 20))
y  <- Dist(c(25, 75))

shannon_mutual_info(xy, x, y)
```

## Conditional Entropy


[Conditional entropy](https://en.wikipedia.org/wiki/Conditional_entropy) quantifies
the amount of information required to describe a random variable $X$ given knowledge
of a random variable $Y$. With $p_Y$ the probability distribution of $Y$, and
$p_{X,Y}$ the joint distribution for $(X,Y)$, the base-$b$ conditional entropy is
defined as
$$
\begin{split}
H(X|Y) &= -\sum_{x,y} p_{X,Y}(x,y) \log_b \frac{p_{X,Y}(x,y)}{p_Y(y)} \\
       &= H(X,Y) - H(Y).
\end{split}
$$  

See [[@Cover1991]](#References) for more details.

__Interface:__

Compute the base-`b` conditional entropy given joint (`p_xy`) and marginal
(`p_y`) distributions.
```r
shannon_conditional_entropy(p_xy, p_y, b = 2)
```

__Examples:__

Compute the base-2 conditional entropy between two random variables:
```{r}
xy <- Dist(c(10, 70, 15, 5))
x  <- Dist(c(80, 20))
y  <- Dist(c(25, 75))

shannon_conditional_entropy(xy, x)
shannon_conditional_entropy(xy, y)
```

## Conditional Mutual Information

## Relative Entropy

## Cross Entropy

# Time Series Measures

The original purpose of [*Inform*](https://github.com/elife-asu/inform/) was to
analyze time series data. This explains why most of
*Inform*'s functionality resides in functions specifically optimized for analyzing time
series. Many information measures have "local" variants which compute a time series of point-wise
values. These feature is directly inherited by the *rinform* wrapper.

We have been meticulous in ensuring that function and parameter names are consistent
across measures. If you notice some inconsistency, please
[report it as an issue](https://github.com/elife-asu/rinform/issue).

## Notation

Throughout the discussion of time series measures, we will try to use a consistent notation.
We will denote random variables as $X,Y,\ldots$, and let $x_i,y_i,\ldots$
represent the $i$-th time step of a time series drawn from the associated random
variable. Many of the measures consider $k$-histories (a.k.a $k$-blocks) of the
time series, e.g.
$$x_i^{(k)} = \left\{x_{i-k+1}, x_{i-k+2},\ldots,x_i\right\}$$.

When denoting probability distributions, we will only make the random variable explicit in
situations where the notation is ambiguous. We will typically write $p(x_i)$,
$p(x_i^{(k)})$, and $p(x_i^{(k)}, x_{i+1})$ to denote the empirical probability
of observing the $x_i$ state, the $x_i^{(k)}$ $k$-history, and the joint
probability of observing $\left(x_i^{(k)}, x_{i+1}\right)$.

## Implementation Details

### The Base: States and Logarithms
The word "base" has two different meanings in the context of information measures on time
series. It could refer to the base of the time series itself, that is the number of unique
states in the time series. For example, the time series $\{0,2,1,0,0\}$ is a base-3
time series. On the other hand, it could refer to the base of the logarithm used in
computing the information content of the inferred probability distributions. The problem is
that these two meanings clash. The base of the time series affects the range of values the
measure can produce, and the base of the logarithm represents a rescaling of those values.

In the *rinform* wrapper we deal with this by *always* using base-2 logarithms and automatically
computing the base of each time series internally to the wrapper. All of this ensures that
the library is a simple as reasonably possible.

### Multiple Initial Conditions
You generally need *a lot* of data to infer a probability distribution.  An experimentalist
or simulator might then collect data over multiple trials or initial conditions. Most of
*rinform*'s time series measures allow the user to pass in a two-dimensional matrix
with each column representing a time series from a different initial condition. From
this the probability distributions are inferred, and the desired value is calculated. This
has the downside of requiring that the user store all of the data in memory, but it has the
advantage of being fast and simple.


## Active Information {#ActiveInformation}

Active information (AI) was introduced in [[@Lizier2012]](#References) to quantify information
storage in distributed computations. Active information is defined in terms of a
temporally local variant

$$
a_{X,i}(k) = \log_2{\frac{p(x_i^{(k)}, x_{i+1})}{p(x_i^{(k)})p(x_{i+1})}},
$$

where the probabilities are constructed empirically from the _entire_ time series.
From the local variant, the temporally global active information is defined as

$$
A_X(k) = \langle a_{X,i}(k) \rangle_i
       = \sum_{x_i^{(k)},x_{i+1}} p(x_i^{(k)},x_{i+1}) \log_2{\frac{p(x_i^{(k)}, x_{i+1})}{p(x_i^{(k)})p(x_{i+1})}}.
$$

Strictly speaking, the local and average active information are defined as

$$
a_{X,i} = \lim_{k\rightarrow \infty}a_{X,i}(k)
\qquad \textrm{and} \qquad
A_X = \lim_{k\rightarrow \infty}A_X(k),
$$

but we do not provide yet limiting functionality in this library
([GitHub issues](https://github.com/elife-asu/Inform/issues/24)).

__Interface:__

Compute the average or local active information with a history length `k`.

```r
active_info(series, k, local = FALSE)
```

__Examples:__

```{r}
# One initial condition:
series <- c(0, 0, 1, 1, 1, 1, 0, 0, 0)
active_info(series, k = 2)

# ..and local variant:
lai <- active_info(series, k = 2, local = T)
t(lai)

# Two initial conditions:
series      <- matrix(nrow = 9, ncol = 2)
series[, 1] <- c(0, 0, 1, 1, 1, 1, 0, 0, 0)
series[, 2] <- c(1, 0, 0, 1, 0, 0, 1, 0, 0)
active_info(series, k = 2)

# ..and local variant:
lai <- active_info(series, k = 2, local = T)
t(lai)

```

## Block Entropy {#BlockEntropy}

Block entropy, also known as $N$-gram entropy [[@Shannon1948]](#References),
is the standard Shannon entropy of the $k$-histories of a time series:
$$
H(X^{(k)}) = -\sum_{x_i^{(k)}} p(x_i^{(k)}) \log_2{p(x_i^{(k)})}
$$
which reduces to the traditional Shannon entropy for $k = 1$.

__Interface:__

Compute the average or local block entropy of a time series with block size `k`.

```r
block_entropy(series, k, local = FALSE)
```
__Examples:__

```{r}
# One initial condition:
# k = 1
series <- c(0, 0, 1, 1, 1, 1, 0, 0, 0)
block_entropy(series, k = 1)

# k = 2
block_entropy(series, k = 2)

# ..and local variant:
# k = 1
be <- block_entropy(series, k = 1, local = T)
t(be)

# k = 2
be <- block_entropy(series, k = 2, local = T)
t(be)

# Two initial conditions:
series      <- matrix(nrow = 9, ncol = 2)
series[, 1] <- c(0, 0, 1, 1, 1, 1, 0, 0, 0)
series[, 2] <- c(1, 0, 0, 1, 0, 0, 1, 0, 0)
block_entropy(series, k = 2)

# ..and local variant:
be <- block_entropy(series, k = 2, local = T)
t(be)
```

## Conditional Entropy

[Conditional entropy](https://en.wikipedia.org/wiki/Conditional_entropy) is a measure of the
amount of information required to describe a random variable $Y$ given knowledge of
another random variable $X$. When applied to time series, two time series are used to
construct the empirical distributions and the conditional entropy is given 
$$
H(Y|X) = - \sum_{x_i,y_i} p(x_i,y_i) \log_2{p(y_i|x_i)}.
$$

This can be viewed as the time-average of the local conditional entropy
$$
h_i(Y|X) = -\log_2{p(y_i|x_i)}.
$$
See [[@Cover1991]](#References) for more information.

__Interface:__

Compute the average and local conditional entropy between two time series.

This function expects the *condition* to be the first argument, `xs`. It is expected that
each time series be the same length `n`, but may have different bases.

```r
conditional_entropy(xs, ys, local = FALSE)
```

__Examples:__

```{r}
xs <- c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1)
ys <- c(0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1)

# Conditional entropy:
conditional_entropy(xs, ys)

conditional_entropy(ys, xs)

# ..and local variant:
ce <- conditional_entropy(xs, ys, local = T)
t(ce)

ce <- conditional_entropy(ys, xs, local = T)
t(ce)
```

## Cross Entropy

[Cross entropy](https://en.wikipedia.org/wiki/Cross_entropy) between two distributions
$p_X$ and $q_X$ measures the amount of information needed to identify events
using a coding scheme optimized for $q_X$ when $p_X$ is the "real" distribution
over $X$.
$$
H(p,q) = -\sum_{x} p(x) \log_2{q(x)}
$$
Cross entropy's local variant is equivalent to the self-information of $q_X$ and as
such is implemented by the [local block entropy](#BlockEntropy).

See [[@Cover1991]](#References) for more details.

__Interface:__

Compute the cross entropy between the "true" and "unnatural" distributions $p_X$ and
$q_X$ from associated time series `ps` and `qs`, respectively.

```r
cross_entropy(ps, qs)
```

__Examples:__

```{r}
ps <- c(0, 1, 1, 0, 1, 0, 0, 1, 0, 0)
qs <- c(0, 0, 0, 0, 0, 1, 1, 0, 0, 1)

cross_entropy(ps, qs)

cross_entropy(qs, ps)
```

## Effective Information {#EffectiveInformation}

Effective information is a _causal_ measure aimed at teasing out the causal structure of a
dynamical system. In essence, it is the mutual information between an "intervention"
distribution — a probability distribution over initial states — and the distribution after
one time step:
$$
EI(A,p) = I(p,p^TA)
$$
where $A$ is the transition probability matrix and $p$ an intervention distribution.
Functionality to construct a transition probability matrix from time series is
provided by the [series_to_tpm](#TimeSeriestToTPM) function.

See [[@Tononi2003]](#References), [[@Hoel2013]](#References) or [[@Hoel2017]](#References)
for more information.

__Interface:__

Compute the effective information from an $n \times n$ transition probability matrix
`tpm` given an intervention distribution `inter`. If `inter` is `NULL`, then the
uniform distribution over the $n$ states is used.

```r
effective_info(tpm, inter = NULL)
```
__Examples:__

Uniform intervention distribution:
```{r}
tpm      <- matrix(0, nrow = 2, ncol = 2)
tpm[, 1] <- c(0.50, 0.5)
tpm[, 2] <- c(0.25, 0.75)
effective_info(tpm, NULL)
```

Non-uniform intervention distribution:
```{r}
tpm      <- matrix(0, nrow = 2, ncol = 2)
tpm[, 1] <- c(0.50, 0.5)
tpm[, 2] <- c(0.25, 0.75)
inter    <- c(0.488372, 0.511628)
effective_info(tpm, inter)
```

## Entropy Rate {#EntropyRate}

[Entropy rate](https://en.wikipedia.org/wiki/Entropy_rate) quantifies the amount of
information needed to describe the next state of $X$ given observations of
$X^{(k)}.$  In other wrods, it is the entropy of the time series conditioned on the
$k$-histories.  The local entropy rate
$$
h_{X,i}(k) = \log_2{\frac{p(x_i^{(k)}, x_{i+1})}{p(x_i^{(k)})}}.
$$
can be averaged to obtain the global entropy rate
$$
H_X(k) = \langle h_{X,i}(k) \rangle_i
       = \sum_{x_i^{(k)},x_{i+1}} p(x_i^{(k)},x_{i+1}) \log_2{\frac{p(x_i^{(k)}, x_{i+1})}{p(x_i^{(k)})}}.
$$

Much as with [active information](#ActiveInformation), the local and average entropy rates are
formally obtained in the limit
$$
h_{X,i} = \lim_{k\rightarrow \infty}h_{X,i}(k)
\qquad \textrm{and} \qquad
H_X = \lim_{k\rightarrow \infty}H_X(k),
$$

but we do not provide yet limiting functionality in this library
([GitHub issues](https://github.com/elife-asu/Inform/issues/24)).

See [[@Cover1991]](#References) for more details.

__Interface:__

Compute the average or local entropy rate with a history length `k`.

```r
entropy_rate(series, k, local = FALSE)
```

__Examples:__

```{r}
# One initial condition:
series <- c(0, 0, 1, 1, 1, 1, 0, 0, 0)
entropy_rate(series, k = 2)

# ..and local variant:
er <- entropy_rate(series, k = 2, local = T)
t(er)

# Two initial conditions:
series      <- matrix(nrow = 9, ncol = 2)
series[, 1] <- c(0, 0, 1, 1, 1, 1, 0, 0, 0)
series[, 2] <- c(1, 0, 0, 1, 0, 0, 1, 0, 0)
entropy_rate(series, k = 2)

# ..and local variant:
er <- entropy_rate(series, k = 2, local = T)
t(er)
```

## Excess Entropy {#ExcessEntropy}

Formally, the excess entropy is the mutual information between two adjacent, semi-infinite
blocks of variables:
$$
E_X = \lim_{k \rightarrow \infty}I[(x_{-k},\ldots,x_{-1}),(x_0,\ldots,x_{k-1})].
$$
Because we cannot take the limit in practice, we implement the finite form:
$$
E_X(k) = I[(x_{-k},\ldots,x_1),(x_0,\ldots,x_{k-1})].
$$

We can think of excess entropy as a slight generalization of [active information](#ActiveInformation) or a special case of [predictive information](#PredictiveInformation).

See [[@Crutchfield2003]](#References) and [[@Feldman2003]](#References) for more details.

__Interface:__

Compute the average or local excess entropy from a time series with block size `k`.

```r
excess_entropy(series, k, local = FALSE)
```

__Examples:__

```{r}
# One initial condition:
series <- c(0, 0, 1, 1, 0, 0, 1, 1, 0)
excess_entropy(series, k = 2)

# ..and local variant:
ee <- excess_entropy(series, k = 2, local = T)
t(ee)

# Two initial conditions:
series      <- matrix(0, nrow = 9, ncol =2)
series[, 1] <- c(0, 0, 1, 1, 0, 0, 1, 1, 0)
series[, 2] <- c(0, 1, 0, 1, 0, 1, 0, 1, 0)
excess_entropy(series, k = 2)

# ..and local variant:
ee <- excess_entropy(series, k = 2, local = T)
t(ee)
```

## Information Flow

## Evidence of Integration

## Mutual Information

[Mutual information](https://en.wikipedia.org/wiki/Mutual_information) (MI) is a measure of
the amount of mutual dependence between at least two random variables. Locally, MI is
defined as
$$
i_i(X_1,\ldots,X_l) = \frac{p(x_{1,i},\ldots,x_{l,i})}{p(x_{1,i})\ldots p(x_{l,i})}.
$$
The mutual information is then just the time average of $i_i(X_1, \ldots, X_l)$:
$$
I(X_1,\ldots,X_l) =
    \sum_{x_{1,i},\ldots,x_{l,i}} p(x_{1,i},\ldots,x_{l,i}) \frac{p(x_{1,i},\ldots,x_{l,i})}{p(x_{1,i})\ldots p(x_{l,i})}.
$$

See [[@Cover1991]](#References) for more details.

__Interface:__

Compute the mutual information or its local variant between two or more time series.
Each variable can have a different base.

```r
mutual_info(series, local = FALSE)
```

__Examples:__

```{r}
# Two variables:
xs <- matrix(c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
               0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1), ncol = 2)

mutual_info(xs)

# ..and local variant:
mi <- mutual_info(xs, local = T)
t(mi)

# Three variables:
xs <- matrix(c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
               0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
               1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1), ncol = 3)

mutual_info(xs)

# ..and local variant:
mi <- mutual_info(xs, local = T)
t(mi)
```

## Partial Information Decomposition

## Predictive Information {#PredictiveInformation}

Formally, the predictive information is the mutual information between a
finite-history and a semi-infinite future:
$$
P_X(k) = \lim_{l \rightarrow \infty}I[(x_{-k},\ldots,x_{-1}),(x_0,\ldots,x_{l-1})].
$$
Of course, we cannot take the limit in practice, so we implement the finite form:
$$
P_X(k,l) = I[(x_{-k},\ldots,x_1),(x_0,\ldots,x_{k-1})].
$$

We can think of [active information](#ActiveInformation) and
[excess entropy](#ExcessEntropy) as a special cases of predictive information.

See [[@Bialek2001a]](#References) and [[@Bialek2001b]](#References) for more details.

__Interface:__

Compute the average or local predictive information from a time series `series` with
history length `kpast` and future length `kfuture`.

```r
predictive_info(series, kpast, kfuture, local = FALSE)
```

__Examples:__

Our examples will compute the predictive information between the current time step and
the next two time steps of a time series, $P_X(1, 2)$.

One initial condition:
```{r}
series <- c(0, 0, 1, 1, 0, 0, 1, 1, 0)
predictive_info(series, 1, 2)

# ..and local variant:
pi <- predictive_info(series, 1, 2, T)
t(pi)
```

Two initial conditions:
```{r}
series      <- matrix(0, nrow = 9, ncol = 2)
series[, 1] <- c(0, 0, 1, 1, 0, 0, 1, 1, 0)
series[, 2] <- c(0, 1, 0, 1, 0, 1, 0, 1, 0)
predictive_info(series, 1, 2)

# ..and local variant:
pi <- predictive_info(series, 1, 2, T)
t(pi)
```

## Relative Entropy

[Relative entropy](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence), also
known as the Kullback-Leibler divergence, measures the amount of information gained in
switching from a prior distribution $q_X$ to a posterior distribution $p_X$ over
_the same support_:
$$
D_{KL}(p||q) = \sum_{x_i} p(x_i) \log_2{\frac{p(x_i)}{q(x_i)}}.
$$
The local counterpart is
$$
d_{KL,i}(p||q) = log_2{\frac{p(x_i)}{q(x_i)}}.
$$
Note that the average in moving from the local to the non-local relative entropy is taken
over the posterior distribution.

See [[@Kullback1951]](#References) and [[@Cover1991]](#References) for more information.

__Interface:__

Compute the average and local relative entropy between time series drawn from posterior and prior
distributions, here `xs` and `ys` respectively.

```r
relative_entropy(xs, ys, local = FALSE)
```

__Examples:__

```{r}
xs <- c(0, 1, 0, 0, 0, 0, 0, 0, 0, 1)
ys <- c(0, 1, 1, 1, 1, 0, 0, 1, 0, 0)

# Average relative entropy:
relative_entropy(xs, ys)

relative_entropy(ys, xs)

# ..and local variant:
re <- relative_entropy(xs, ys, local = T)
t(re)

re <- relative_entropy(ys, xs, local = T)
t(re)
```

## Separable Information

## Transfer Entropy

[Transer entropy](https://en.wikipedia.org/wiki/Transfer_entropy) (TE) was introduced
in [[@Schreiber2000]](#References) to quantify information transfer between an
information source and a destination, conditioning out shared history effects. TE
was originally formulated considering only the source and destination; however many systems
of interest have more than just those two components. As such, it may be further necessary to
condition the probabilities on the states of all "background" components in the system.
These two forms are sometimes referred to as _apparent_ and _complete_ transfer entropy,
respectively ([[Lizier2008]](#References)).

Our implementation of TE allows the user to condition the probabilities on any number of
background processes, within hardware limits. For the subsequent description, take $X$
to be the source, $Y$ the target, $W = \{W_1, \ldots, W_l\}$ be the background processes
against which we'd like to condition. For example, we might take the state of two nodes
in a dynamical network as the source and target, while all other nodes in the network
are treated as the background. Transfer entropy is then defined in terms of a
time-local variant
$$
t_{X \rightarrow Y, \mathcal{W}, i}(k) =
    \log_2{\frac{p(y_{i+1},x_i|y^{(k)}_i,W_i)}
    {p(y_{i+1}|y^{(k)}_i,W_i)p(x_{i+1}|y^{(k)}_i,W_i)}}
$$

where $W_i = w_{1,i}, \ldots, w_{l,i}$ are the states of each of the background nodes at
time step $i$, and the probabilities are constructed empirically from the _entire_ time
series. From the local variant, the temporally global transfer entropy is defined as
$$
T_{X \rightarrow Y, \mathcal{W}}(k)
    = \langle t_{X \rightarrow Y, \mathcal{W}, i}(k) \rangle_i
    = \sum_{y_{i+1},y^{(k)},x_i,W_i} p(y_{i+1},y^{(k)}_i,x_i,W_i)
    \log_2{\frac{p(y_{i+1},x_i|y^{(k)}_i,W_i)}
    {p(y_{i+1}|y^{(k)}_i,W_i)p(x_{i+1}|y^{(k)}_i,W_i)}}.
$$

Strictly speaking, the local and average transfer entropy are defined as
$$
t_{X \rightarrow Y, \mathcal{W}, i}
    = \lim_{k\rightarrow \infty} t_{X \rightarrow Y, \mathcal{W}, i}(k)
\qquad \textrm{and} \qquad
T_{X \rightarrow Y, \mathcal{W}}
    = \lim_{k\rightarrow \infty} t_{X \rightarrow Y, \mathcal{W}}(k),
$$

but we do not provide yet limiting functionality in this library
([GitHub issues](https://github.com/elife-asu/Inform/issues/24)).

__Interface:__

Compute the average or local transfer entropy with a history length `k` with
the possibility to conditioning on the background `back`.

```r
transfer_entropy(ys, xs, ws = NULL, k, local = FALSE) 
```

__Examples:__

```{r}
# One initial condition, no background:
xs <- c(0, 1, 1, 1, 1, 0, 0, 0, 0)
ys <- c(0, 0, 1, 1, 1, 1, 0, 0, 0)
transfer_entropy(xs, ys, ws = NULL, k = 2)

# .. and local variant:
te <- transfer_entropy(xs, ys, ws = NULL, k = 2, local = T)
t(te)

# Two initial conditions, no background:
xs <- matrix(0, nrow = 9, ncol = 2)
xs[, 1] <- c(1, 0, 0, 0, 0, 1, 1, 1, 1)
xs[, 2] <- c(1, 1, 1, 1, 0, 0, 0, 1, 1)
ys <- matrix(0, nrow = 9, ncol = 2)
ys[, 1] <- c(0, 0, 1, 1, 1, 1, 0, 0, 0)
ys[, 2] <- c(1, 0, 0, 0, 0, 1, 1, 1, 0)
transfer_entropy(xs, ys, ws = NULL, k = 2)

# .. and local variant:
te <- transfer_entropy(xs, ys, ws = NULL, k = 2, local = T)
t(te)

# One initial condition, one background process:
xs <- c(0, 1, 1, 1, 1, 0, 0, 0, 0)
ys <- c(0, 0, 1, 1, 1, 1, 0, 0, 0)
ws <- array(c(0, 1, 1, 1, 1, 0, 1, 1, 1), dim = c(1, 9, 1))
transfer_entropy(xs, ys, ws, k = 2)

# .. and local variant:
te <- transfer_entropy(xs, ys, ws, k = 2, local = T)
t(te)
```

The following example is interesting in that the two background processes
provide the same information about the destination as the source process does
($X = W_1 \oplus W_2$) but they don't separately.

```{r}
# One initial condition, two background processes:
xs <- c(0, 1, 1, 1, 1, 0, 0, 0, 0)
ys <- c(0, 0, 1, 1, 1, 1, 0, 0, 0)
ws <- array(c(1, 0, 1, 0, 1, 1, 1, 1, 1,
              1, 1, 0, 1, 0, 1, 1, 1, 1), dim = c(2, 9, 1))
transfer_entropy(xs, ys, ws, k = 2)

# .. and local variant:
te <- transfer_entropy(xs, ys, ws, k = 2, local = T)
t(te)
```

# Utilities

+--------------------+----------------------------------------------------------+
| Type               | Functions                                                |
+====================+==========================================================+
| Binning            |[`series_range`](#SeriesRange), [`bin_series`](#BinSeries)|
+--------------------+----------------------------------------------------------+
| Black-Boxing       |  |
+--------------------+---------------------------------------------------------------+
| Coalescing         | [`coalesce`](#Coalesce) |
+--------------------+----------------------------------------------------------+
| State Encoding     | [`encode`](#Encode), [`decode`](#Decode) |
+--------------------+----------------------------------------------------------+
| Random Time Series |            |
+--------------------+----------------------------------------------------------+
| Time Series to TPM |            |
+--------------------+----------------------------------------------------------+


## Binning Time Series

All of the currently implemented time series measures are only defined
on discretely-valued time series. However, in practice continuously-valued
time series are ubiquitous. There are two approaches to accomodating continuous values.

The simplest is to bin the time series, forcing the values into discrete states.
This method has its downsides, namely that the binning is often a bit unphysical
and it can introduce bias. What’s more, without some kind of guiding principle it
can be difficult to decide exactly which binning approach.

The second approach attempts to infer condinuous probability distributions
from continuous data. This is potentially more robust, but more technically d
ifficult. Unfortunately, _rinform_ does not yet have an implementation of information
measures on continous distributions.


### Series Range {#SeriesRange}

__Interface:__

Compute the range, minimum and maximum values in a floating-point time series
and return them as a list.

```r
series_range(series)
```

__Examples:__

```{r}
# Valid time series
xs <- c(0.2, 0.5, -3.2, 1.8, 0.6, 2.3)
series_range(xs)
```

### Bin Series {#BinSeries}

__Interface:__

Bin a floating-point time series following one of three different modalities:

* Bin a floating-point time series into a finite-state time series with `b` uniform sized
bins, and return the size of the bins. If the size of each bin is too small, less than
$10 \epsilon$, then all entries are placed in the same bin and an error is set.
($\epsilon$ is the double-precision machine epsilon.)

* Bin a floating-point time series into bins of a specified size `step`, and return
the number of bins. If the bin size is too small, less than $10 \epsilon$, then an error
is set.

* Bin a floating-point time series into bins defined by an array of `bounds`, and
return the bounds of bins. 

Return a list giving the binned sequence, the number of bins and either the
bin sizes or bin bounds.
     
```r
bin_series(series, b = NA, step = NA, bounds = NA)
```

__Examples:__

```{r}
# First method: number of bins
series <- c(1, 2, 3, 4, 5, 6)
bin_series(series, b = 2)

# Second method: bin size
bin_series(series, step = 2.0)

# Third method: bin bounds
bin_series(series, bounds = c(3,7))
```

## Black-Boxing Time Series

### Black-Box {#BlackBox}

__Interface:__

__Examples:__

### Black-Box Parts {#BlackBoxParts}

__Interface:__

__Examples:__

## Coalescing Time Series {#Coalesce}
    
The magic of information measures is that the actual values of a time series
are irrelavent. For example, $\{0, 1, 0, 1, 1 \}$ has the same entropy as
$\{2, 9, 2, 9, 9\}$ (possibly up to a rescaling). This give us the freedom
to shift around the values of a time series as long as we do not change the
relative number of states.
    
This function thus provides a way of "compressing" a time series into as
small a base as possible. Why is this useful? Many of the measures use the base
of the time series to determine how much memory to allocate; the larger the base,
the higher the memory usage. It also affects the overall performance as the combinatorics
climb exponentially with the base.
    
Notice that the encoding that is used ensures that the ordering of the states
stays the same, e.g.,
$$
\{-8 \rightarrow 0, -2 \rightarrow 1, 2 \rightarrow 2, 4 \rightarrow 3, 6 \rightarrow 4\}
$$.

__Interface:__

Coalesce a timeseries into as few contiguous states as possible.
```r
coalesce(series)
```

__Examples:__

The two standard usage cases for this function are to reduce the base of a
time series:
```{r}
coalesce(c(0, 2, 0, 2, 0, 2))
```
and to ensure that the states are non-negative:
```{r}
coalesce(c(-8, 2, 6, -2, 4))
```

## State Encoding

State encoding is a necessity when complex time series are being analyzed.
For example, `k`-history must be encoded as an integer in order to “observe”
it using a Dist. What if you are interested in correlating the aggragate
state of one group of nodes with that of another? You’d need to encode
the groups’ states as integers. The following functions provides such
functionality.

<div class="alert alert-warning">
  <strong>Attention!</strong>

  As a practical matter, these utility functions should only be used as a
  stop-gap while a solution for your problem is implemented in the core
  _Inform_ library and wrapped in _rinform_. “Why?” you ask? Well, these
  functions are about as efficient as they can be for one-off state encodings,
  but most of the time you are interested in encoding sequences of states.
  This can be done much more efficiently if you encode the entire sequence
  at once. You need domain-specific information to make that happen.
</div>

### Encode {#Encode}
    
This function uses a [big-endian](https://en.wikipedia.org/wiki/Endianness#Examples)
encoding scheme. That is, the most significant bits of the encoded integer
are determined by the left-most end of the unencoded state. If `b` is not provided
(or is `NA`), the base is inferred from the state with a minimum value of 2.

__Interface:__

Encode a base-'b' array of integers into a single integer.
```r
encode(state, b = NA)
```

__Examples:__

Using the big-endian encoding scheme, the most significant bits of the encoded integer
are determined by the left-most end of the unencoded state:
```{r}
encode(c(0, 0, 1), b = 2)
encode(c(0, 1, 0), b = 3)
encode(c(1, 0, 0), b = 4)
encode(c(1, 0, 4), b = 5)
```

If `b` is not provided or is `NA`, itt is inferred from the state:
```{r}
encode(c(0, 0, 2), NA)
encode(c(0, 2, 0))
encode(c(1, 2, 1))
```

### Decode {#Decode}
    
The provided encoded state is decoded using the
[big-endian](https://en.wikipedia.org/wiki/Endianness#Examples) encoding
scheme. Note that the base `b` must be provided, but the number of digits `n` is
optional. If `n` is provided then the decoded state will have exactly that
many elements. 
    
__Interface:__

Decode an integer into a base-`b` array with `n` digits.
```r
decode(encoding, b, n = NA)
```

__Examples:__

This function assumes a big-endian encoding scheme:
```{r}
decode(2, b = 2, n = 2)
# array([1, 0], dtype=int32)
decode(6, b = 2, n = 3)
#array([1, 1, 0], dtype=int32)
decode(6, b = 3, n = 2)
#array([2, 0], dtype=int32)
```

If `n` is provided then the decoded state will have exactly that
many elements. However, note that if the provided `n` is too small to contain a full
representation of the state, an error will be raised.
```{r}
decode(2, b = 2, n = 4)
```

If `n` is not provided, the length of the decoded state is inferred
from the `encoding` to be as small as possible:
```{r}
decode(1, b = 2)
decode(1, b = 3)
decode(3, b = 2)
decode(3, b = 3)
decode(3, b = 4)
```

## Partitioning Time Series

__Interface:__

__Examples:__

## Random Time Series

__Interface:__

__Examples:__

## Time Series to TPM {#TimeSeriestToTPM}

__Interface:__
series_to_tpm

__Examples:__

# References {#References}