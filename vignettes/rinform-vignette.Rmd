---
title: "The rinform Wrapper"
author: "Gabriele Valentini"
date: "`r Sys.Date()`"
output:
#  rmarkdown::html_vignette
  html_document:
    toc: true # table of content true
    toc_depth: 2  # upto three depths of headings (specified by #, ## and ###)
    toc_float: true    
    number_sections: true  ## if you want number sections at each table header
    theme: yeti  # many options for theme, this one is my favorite.
    highlight: tango  # specifies the syntax highlighting style
  pdf_document:
    highlight: null
    number_sections: yes    
bibliography: vignette.bibtex
vignette: >
  %\VignetteIndexEntry{The rinform Wrapper}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

An R wrapper of the [Inform v1.0.0](https://elife-asu.github.io/Inform/) C library for performing information analysis of complex system. As for the Inform library, _rinform_ is structured around the concepts of:

* discrete emperical probability distributions which form the basis for
  all of the information-theoretic measures,
  
* classic information-theoretic measures built upon empirical distributions,

* measures of information dynamics for time series.

In addition to the core components, rinform also provides a small collection of utilities to deal with time series. 


# Getting Started

# Error Handling

# Empirical Distributions


The [`Dist`](#DistClass) class provides an _empirical_ distribution,
i.e. a histogram, representing the observed frequencies of some fixed-sized set of events.
This class is the basis for all of the fundamental information measures provided on
discrete probability distributions.

The purpose of the [`Dist`](#DistClass) class is to make writing custom
implementations of information-theoretic measures possible. Most of the measures implemented
in _rinform_ use this structure at their core.

+--------------------+----------------------------------------------------------+
| Type               | Functions                                                |
+====================+==========================================================+
| Class              |[`Dist`](#DistClass)                                      |
+--------------------+----------------------------------------------------------+
| Allocation         | [`resize`](#Resize), [`copy`](#Copy), [`infer`](#Infer), [`approximate`](#Approximate), [`uniform`](#Uniform) |
+--------------------+---------------------------------------------------------------+
| Accessors/Mutators | [`length`](#length), [`counts`](#Counts), [`valid`](#Valid), [`get_item`](#GetItem), [`set_item`](#SetItem), [`tick`](#Tick), [`accumulate`](#Accumulate) |
+--------------------+----------------------------------------------------------+
| Probabilities      |[`probability`](#Probability), [`dump`](#Dump)            |
+--------------------+----------------------------------------------------------+

## Distribution Class {#DistClass}

This class is the basis for almost all of the calculations performed via the library. It
provides a means of managing observations and converting the observed event frequencies to
probabilities.

The distribution is, roughly, a histogram with finite support. The events are assumed to be
mapped to the dense set of integers $\{0,1,\ldots,N-1\}$ where $N$ is the number
of observable events. The number of observable events can be extracted with
[`length`](#length).

Whenever the size of the support or the number of observed events is zero, the distribution
is considered invalid meaning that you can't trust any probabilities extracted from it.

__Interface:__

A distribution of observed event frequencies. If the parameter `n` is an integer,
the distribution is constructed with a zeroed support of size $n$. If `n` is a vector
of integer values, the sequence is treated as the underlying support. On the other hand,
if `n` is a vector of floating point values, it is treated as a probability distribution
and must sum to unity. Note, if a probability distribution is given as the underlying
support, it will first be converted to a histogram with a precision of 9 significant figures.

```r
Dist(n)
```

__Examples:__

```{r}
# Create an empty distribution with support size 3
Dist(3)

# Create a distribution with 2 events, the first observed 5 times, the second
# observed 3 times
Dist(c(5, 3))
```

## Allocation

### Resize {#Resize}

If the distribution is `NULL`, a new distribution of the requested size is created.

If the distribution already has the requested size, then the original distribution is
returned. If the requested size is greater than the current size, then the newly observable
events are assigned zero frequency. If the requested size is less than the current size,
then the discarded events are lost and the observation count is adjusted accordingly.

Note that the distribution cannot be reduced to size `0`. In that case the distribution is
left unmodified.

__Interface:__

Resize the distribution `d` to have new support of size `n`.

```r
resize(d, n)
```
__Examples:__

```{r}
# Create a distribution with size 2
d <- Dist(c(2, 13))
d

# Increase the size of the support to 5
d <- resize(d, 5)
d

# Decrease the size of the support to 3
d <- resize(d, 3)
d
```

### Copy {#Copy}

If the source distribution is `NULL`, then `NULL` is returned.

__Interface:__

Return a copy of the distribution `d`.

```r
copy(d)
```

__Examples:__

```{r}
# Create base distribution
d <- Dist(c(1:5))
d

# Copy distribution
p <- copy(d)
p
```

### Infer {#Infer}

__Interface:__

Infer a distribution from a collection of observed `events`.

```r
infer(events)
```

__Examples:__

```{r}
# Create a distribution ~ {3/5, 2/5}
dist <- infer(c(0, 0, 1, 0, 1))
dump(dist)

# Create a distribution ~ {3/8, 3/8, 2/8}
dist <- infer(c(0, 0, 1, 0, 1, 2, 2, 1))
dump(dist)
```

### Approximate {#Approximate}

__Interface:__

Approximate a given probability distribution to a given tolerance.

```r
approximate(probs, tol)
```

__Examples:__

```{r}
# Approximate a distribution with tolerance 1e-3
probs <- c(0.5, 0.2, 0.3)
dist <- approximate(probs, 1e-3)
dist$histogram

probs <- c(1./3, 2./3)
dist <- approximate(probs, 1e-3)
dist$histogram
```

### Uniform {#Uniform}

__Interface:__

Create a uniform distribution of a given size `n`. The support size `n` must
define a valid support (i.e., `n` must be greater than 0).

```r
uniform(n)
```
__Examples:__

```{r}
# Uniform distribution of size 3
dist <- uniform(3)
dist

dump(dist)
```

## Accessors/Mutators

### Length {#length}

__Interface:__

Get the size of the distribution's support. If the distribution is `NULL`, then a support
of 0 is returned.

```r
length(d)
```

__Examples:__

```{r}
# NULL distribution
d <- NULL
length(d)

# Distribution with size 5
d <- Dist(5)
length(d)
```

### Counts {#Counts}

__Interface:__

Get the total number of observations so far made.

```
counts(d)
```

__Examples:__

```{r}
# Counts of valid distribution
dist <- Dist(c(5, 10))
counts(dist) == 15

dist <- set_item(dist, 2, 5)
counts(dist) == 10
```

### Valid {#Valid}

__Interface:__

Determine whether or not the distribution is valid. In order to safely extract probabilities,
the size of the support must be non-zero and the number of observations must be non-zero. In any
other case, the distribution is invalid.

```r
valid(d)
```

__Examples:__

```{r}
# Distribution with 0 observations
dist <- Dist(3)
valid(dist)

# Valid distribution
dist <- Dist(c(1:5))
valid(dist)

# Invalid distribution with 0 support
dist$size <- as.integer(0)
valid(dist)
```

### Get Item {#GetItem}

__Interface:__

Get the number of occurrences of a given `event`.

```r
get_item(d, event)
```

__Examples:__

```{r}
# Get item from a valid distribution
dist <- Dist(c(3, 2, 1, 0))
get_item(dist, 1) == 3
get_item(dist, 2) == 2
get_item(dist, 3) == 1
get_item(dist, 4) == 0
```

### Set Item {#SetItem}

__Interface:__

Set the number of occurrences of a given `event`. This function manually sets the number
of occurrences of a given `event`.  Note that the only
restriction is that the value be positive. This means that this function can be used to
invalidate the distribution by changing all of the event frequencies to zero.

```r
set_item(d, event, value)
```

__Examples:__

```{r}
# Initialize empty distribution
dist <- Dist(2)
dist

# Set item into a valid distribution
dist <- set_item(dist, 1, 3)
dist <- set_item(dist, 2, 8)
dist
```

### Tick {#Tick}

__Interface:__

Increment the number of observations of a given `event` and return an updated
copy of the distribution `d`. As an alternative to [`set_item`](#SetItem), this
function simply increments the number of occurrences of a given event. This
is useful when iteratively observing events.

```r
tick(d, event)
```

__Examples:__

```{r}
# Initial distribution
dist <- Dist(c(2, 4))

# Adding an observation for each event
dist <- tick(dist, 1)
get_item(dist, 1) == 3

dist <- tick(dist, 2)
get_item(dist, 2) == 5
```

### Accumulate {#Accumulate}

__Interface:__

Accumulate observations from a series. If an invalid distribution is provided, no
events will be observed and an error will be raised. If an invalid event is
provided, then the number of valid events to that point will be added and a
warning will be raised.

```r
accumulate(d, events)
```

__Examples:__

```{r}
# Create a valid distribution
d <- Dist(c(1, 2, 3))
dump(d)

# Accumulate events
d <- accumulate(d, events)
dump(d)

# Accumulate invalid events
events <- c(0, 1, 1, 3, 1)
d <- accumulate(d, events)
dump(d)
```

## Probabilities

### Probability {#Probability}

__Interface:__

Extract the probability of an `event` from a distribution `d`. This function simply computes the
probability of a given `event` and returns that value. 

```r
probability(d, event)
```

__Example:__

```{r}
# Initialize distribution
dist <- Dist(c(2, 2, 4))

# Compute probabilities
probability(dist, 1) == 0.25
probability(dist, 2) == 0.25
probability(dist, 3) == 0.50
```

### Dump {#Dump}

__Interface:__

Dump the probabilities of all events to an array. This function computes the
probabilities of all of the events in the support and return them as an array.

```r
dump(d)
```

__Example:__

```{r}
# Initialize distribution and dump probabilities
dist <- Dist(c(2, 2, 4))
dump(dist)

# Modify and dump again
dist <- set_item(dist, 1, 12)
dump(dist)
```

# Shannon Information Measures

## Entropy

## Mutual Information

## Conditional Entropy

## Conditional Mutual Information

## Relative Entropy

## Cross Entropy

# Time Series Measures

The original purpose of [*Inform*](https://github.com/elife-asu/inform/) was to
analyze time series data. This explains why most of
*Inform*'s functionality resides in functions specifically optimized for analyzing time
series. Many information measures have "local" variants which compute a time series of point-wise
values. These feature is directly inherited by the *rinform* wrapper.

We have been meticulous in ensuring that function and parameter names are consistent
across measures. If you notice some inconsistency, please
[report it as an issue](https://github.com/elife-asu/rinform/issue).

## Notation

Throughout the discussion of time series measures, we will try to use a consistent notation.
We will denote random variables as $X,Y,\ldots$, and let $x_i,y_i,\ldots$
represent the $i$-th time step of a time series drawn from the associated random
variable. Many of the measures consider $k$-histories (a.k.a $k$-blocks) of the
time series, e.g.
$$x_i^{(k)} = \left\{x_{i-k+1}, x_{i-k+2},\ldots,x_i\right\}$$.

When denoting probability distributions, we will only make the random variable explicit in
situations where the notation is ambiguous. We will typically write $p(x_i)$,
$p(x_i^{(k)})$, and $p(x_i^{(k)}, x_{i+1})$ to denote the empirical probability
of observing the $x_i$ state, the $x_i^{(k)}$ $k$-history, and the joint
probability of observing $\left(x_i^{(k)}, x_{i+1}\right)$.

## Implementation Details

### The Base: States and Logarithms
The word "base" has two different meanings in the context of information measures on time
series. It could refer to the base of the time series itself, that is the number of unique
states in the time series. For example, the time series $\{0,2,1,0,0\}$ is a base-3
time series. On the other hand, it could refer to the base of the logarithm used in
computing the information content of the inferred probability distributions. The problem is
that these two meanings clash. The base of the time series affects the range of values the
measure can produce, and the base of the logarithm represents a rescaling of those values.

In the *rinform* wrapper we deal with this by *always* using base-2 logarithms and automatically
computing the base of each time series internally to the wrapper. All of this ensures that
the library is a simple as reasonably possible.

### Multiple Initial Conditions
You generally need *a lot* of data to infer a probability distribution.  An experimentalist
or simulator might then collect data over multiple trials or initial conditions. Most of
*rinform*'s time series measures allow the user to pass in a two-dimensional matrix
with each column representing a time series from a different initial condition. From
this the probability distributions are inferred, and the desired value is calculated. This
has the downside of requiring that the user store all of the data in memory, but it has the
advantage of being fast and simple.


## Active Information {#ActiveInformation}

Active information (AI) was introduced in [[@Lizier2012]](#References) to quantify information
storage in distributed computations. Active information is defined in terms of a
temporally local variant

$$
a_{X,i}(k) = \log_2{\frac{p(x_i^{(k)}, x_{i+1})}{p(x_i^{(k)})p(x_{i+1})}},
$$

where the probabilities are constructed empirically from the _entire_ time series.
From the local variant, the temporally global active information is defined as

$$
A_X(k) = \langle a_{X,i}(k) \rangle_i
       = \sum_{x_i^{(k)},x_{i+1}} p(x_i^{(k)},x_{i+1}) \log_2{\frac{p(x_i^{(k)}, x_{i+1})}{p(x_i^{(k)})p(x_{i+1})}}.
$$

Strictly speaking, the local and average active information are defined as

$$
a_{X,i} = \lim_{k\rightarrow \infty}a_{X,i}(k)
\qquad \textrm{and} \qquad
A_X = \lim_{k\rightarrow \infty}A_X(k),
$$

but we do not provide yet limiting functionality in this library
([GitHub issues](https://github.com/elife-asu/Inform/issues/24)).

__Interface:__

Compute the average or local active information with a history length `k`.

```r
active_info(series, k, local = FALSE)
```

__Examples:__

```{r}
# One initial condition:
series <- c(0, 0, 1, 1, 1, 1, 0, 0, 0)
active_info(series, k = 2)

# ..and local variant:
lai <- active_info(series, k = 2, local = T)
t(lai)

# Two initial conditions:
series      <- matrix(nrow = 9, ncol = 2)
series[, 1] <- c(0, 0, 1, 1, 1, 1, 0, 0, 0)
series[, 2] <- c(1, 0, 0, 1, 0, 0, 1, 0, 0)
active_info(series, k = 2)

# ..and local variant:
lai <- active_info(series, k = 2, local = T)
t(lai)

```

## Block Entropy {#BlockEntropy}

Block entropy, also known as $N$-gram entropy [[@Shannon1948]](#References),
is the standard Shannon entropy of the $k$-histories of a time series:
$$
H(X^{(k)}) = -\sum_{x_i^{(k)}} p(x_i^{(k)}) \log_2{p(x_i^{(k)})}
$$
which reduces to the traditional Shannon entropy for $k = 1$.

__Interface:__

Compute the average or local block entropy of a time series with block size `k`.

```r
block_entropy(series, k, local = FALSE)
```
__Examples:__

```{r}
# One initial condition:
# k = 1
series <- c(0, 0, 1, 1, 1, 1, 0, 0, 0)
block_entropy(series, k = 1)

# k = 2
block_entropy(series, k = 2)

# ..and local variant:
# k = 1
be <- block_entropy(series, k = 1, local = T)
t(be)

# k = 2
be <- block_entropy(series, k = 2, local = T)
t(be)

# Two initial conditions:
series      <- matrix(nrow = 9, ncol = 2)
series[, 1] <- c(0, 0, 1, 1, 1, 1, 0, 0, 0)
series[, 2] <- c(1, 0, 0, 1, 0, 0, 1, 0, 0)
block_entropy(series, k = 2)

# ..and local variant:
be <- block_entropy(series, k = 2, local = T)
t(be)
```

## Conditional Entropy

[Conditional entropy](https://en.wikipedia.org/wiki/Conditional_entropy) is a measure of the
amount of information required to describe a random variable $Y$ given knowledge of
another random variable $X$. When applied to time series, two time series are used to
construct the empirical distributions and the conditional entropy is given 
$$
H(Y|X) = - \sum_{x_i,y_i} p(x_i,y_i) \log_2{p(y_i|x_i)}.
$$

This can be viewed as the time-average of the local conditional entropy
$$
h_i(Y|X) = -\log_2{p(y_i|x_i)}.
$$
See [[@Cover1991]](#References) for more information.

__Interface:__

Compute the average and local conditional entropy between two time series.

This function expects the *condition* to be the first argument, `xs`. It is expected that
each time series be the same length `n`, but may have different bases.

```r
conditional_entropy(xs, ys, local = FALSE)
```

__Examples:__

```{r}
xs <- c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1)
ys <- c(0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1)

# Conditional entropy:
conditional_entropy(xs, ys)

conditional_entropy(ys, xs)

# ..and local variant:
ce <- conditional_entropy(xs, ys, local = T)
t(ce)

ce <- conditional_entropy(ys, xs, local = T)
t(ce)
```

## Cross Entropy

[Cross entropy](https://en.wikipedia.org/wiki/Cross_entropy) between two distributions
$p_X$ and $q_X$ measures the amount of information needed to identify events
using a coding scheme optimized for $q_X$ when $p_X$ is the "real" distribution
over $X$.
$$
H(p,q) = -\sum_{x} p(x) \log_2{q(x)}
$$
Cross entropy's local variant is equivalent to the self-information of $q_X$ and as
such is implemented by the [local block entropy](#BlockEntropy).

See [[@Cover1991]](#References) for more details.

__Interface:__

Compute the cross entropy between the "true" and "unnatural" distributions $p_X$ and
$q_X$ from associated time series `ps` and `qs`, respectively.

```r
cross_entropy(ps, qs)
```

__Examples:__

```{r}
ps <- c(0, 1, 1, 0, 1, 0, 0, 1, 0, 0)
qs <- c(0, 0, 0, 0, 0, 1, 1, 0, 0, 1)

cross_entropy(ps, qs)

cross_entropy(qs, ps)
```

## Effective Information

## Entropy Rate

[Entropy rate](https://en.wikipedia.org/wiki/Entropy_rate) quantifies the amount of
information needed to describe the next state of $X$ given observations of
$X^{(k)}.$  In other wrods, it is the entropy of the time series conditioned on the
$k$-histories.  The local entropy rate
$$
h_{X,i}(k) = \log_2{\frac{p(x_i^{(k)}, x_{i+1})}{p(x_i^{(k)})}}.
$$
can be averaged to obtain the global entropy rate
$$
H_X(k) = \langle h_{X,i}(k) \rangle_i
       = \sum_{x_i^{(k)},x_{i+1}} p(x_i^{(k)},x_{i+1}) \log_2{\frac{p(x_i^{(k)}, x_{i+1})}{p(x_i^{(k)})}}.
$$

Much as with [active information](#ActiveInformation), the local and average entropy rates are
formally obtained in the limit
$$
h_{X,i} = \lim_{k\rightarrow \infty}h_{X,i}(k)
\qquad \textrm{and} \qquad
H_X = \lim_{k\rightarrow \infty}H_X(k),
$$

but we do not provide yet limiting functionality in this library
([GitHub issues](https://github.com/elife-asu/Inform/issues/24)).

See [[@Cover1991]](#References) for more details.

__Interface:__

Compute the average or local entropy rate with a history length `k`.

```r
entropy_rate(series, k, local = FALSE)
```

__Examples:__

```{r}
# One initial condition:
series <- c(0, 0, 1, 1, 1, 1, 0, 0, 0)
entropy_rate(series, k = 2)

# ..and local variant:
er <- entropy_rate(series, k = 2, local = T)
t(er)

# Two initial conditions:
series      <- matrix(nrow = 9, ncol = 2)
series[, 1] <- c(0, 0, 1, 1, 1, 1, 0, 0, 0)
series[, 2] <- c(1, 0, 0, 1, 0, 0, 1, 0, 0)
entropy_rate(series, k = 2)

# ..and local variant:
er <- entropy_rate(series, k = 2, local = T)
t(er)
```

## Excess Entropy

## Information Flow

## Evidence of Integration

## Mutual Information

[Mutual information](https://en.wikipedia.org/wiki/Mutual_information) (MI) is a measure of
the amount of mutual dependence between at least two random variables. Locally, MI is
defined as
$$
i_i(X_1,\ldots,X_l) = \frac{p(x_{1,i},\ldots,x_{l,i})}{p(x_{1,i})\ldots p(x_{l,i})}.
$$
The mutual information is then just the time average of $i_i(X_1, \ldots, X_l)$:
$$
I(X_1,\ldots,X_l) =
    \sum_{x_{1,i},\ldots,x_{l,i}} p(x_{1,i},\ldots,x_{l,i}) \frac{p(x_{1,i},\ldots,x_{l,i})}{p(x_{1,i})\ldots p(x_{l,i})}.
$$

See [[@Cover1991]](#References) for more details.

__Interface:__

<<<<<<< HEAD
Compute the mutual information or its local variant between two or more time series.
Each variable can have a different base.

```r
mutual_info(series, local = FALSE)
```

__Examples:__

```{r}
# Two variables:
xs <- matrix(c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
               0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1), ncol = 2)

mutual_info(xs)

# ..and local variant:
mi <- mutual_info(xs, local = T)
t(mi)

# Three variables:
xs <- matrix(c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
               0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
               1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1), ncol = 3)

mutual_info(xs)

# ..and local variant:
mi <- mutual_info(xs, local = T)
t(mi)
```

## Partial Information Decomposition

## Predictive Information

## Relative Entropy

[Relative entropy](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence), also
known as the Kullback-Leibler divergence, measures the amount of information gained in
switching from a prior distribution $q_X$ to a posterior distribution $p_X$ over
_the same support_:
$$
D_{KL}(p||q) = \sum_{x_i} p(x_i) \log_2{\frac{p(x_i)}{q(x_i)}}.
$$
The local counterpart is
$$
d_{KL,i}(p||q) = log_2{\frac{p(x_i)}{q(x_i)}}.
$$
Note that the average in moving from the local to the non-local relative entropy is taken
over the posterior distribution.

See [[@Kullback1951]](#References) and [[@Cover1991]](#References) for more information.

__Interface:__

Compute the average and local relative entropy between time series drawn from posterior and prior
distributions, here `xs` and `ys` respectively.

```r
relative_entropy(xs, ys, local = FALSE)
```

__Examples:__

```{r}
xs <- c(0, 1, 0, 0, 0, 0, 0, 0, 0, 1)
ys <- c(0, 1, 1, 1, 1, 0, 0, 1, 0, 0)

# Average relative entropy:
relative_entropy(xs, ys)

relative_entropy(ys, xs)

# ..and local variant:
re <- relative_entropy(xs, ys, local = T)
t(re)

re <- relative_entropy(ys, xs, local = T)
t(re)
```

## Separable Information

## Transfer Entropy

# Utilities

+--------------------+----------------------------------------------------------+
| Type               | Functions                                                |
+====================+==========================================================+
| Binning            |[`series_range`](#SeriesRange), [`bin_series`](#BinSeries)|
+--------------------+----------------------------------------------------------+
| Black-Boxing       |  |
+--------------------+---------------------------------------------------------------+
| Coalescing         |  |
+--------------------+----------------------------------------------------------+
| Encoding/Decoding  |            |
+--------------------+----------------------------------------------------------+
| Random Time Series |            |
+--------------------+----------------------------------------------------------+
| Time Series to TPM |            |
+--------------------+----------------------------------------------------------+


## Binning Time Series

### Series Range {#SeriesRange}

__Interface:__

Compute the range, minimum and maximum values in a floating-point time series
and return them as a list.

```r
series_range(series)
```

__Examples:__

```{r}
# Valid time series
xs <- c(0.2, 0.5, -3.2, 1.8, 0.6, 2.3)
series_range(xs)
```

### Bin Series {#BinSeries}

__Interface:__

Bin a floating-point time series following one of three different modalities:

* Bin a floating-point time series into a finite-state time series with `b` uniform sized
bins, and return the size of the bins. If the size of each bin is too small, less than
$10 \epsilon$, then all entries are placed in the same bin and an error is set.
($\epsilon$ is the double-precision machine epsilon.)

* Bin a floating-point time series into bins of a specified size `step`, and return
the number of bins. If the bin size is too small, less than $10 \epsilon$, then an error
is set.

* Bin a floating-point time series into bins defined by an array of `bounds`, and
return the bounds of bins. 

Return a list giving the binned sequence, the number of bins and either the
bin sizes or bin bounds.
     
```r
bin_series(series, b = NA, step = NA, bounds = NA)
```

__Examples:__

```{r}
# First method: number of bins
series <- c(1, 2, 3, 4, 5, 6)
bin_series(series, b = 2)

# Second method: bin size
bin_series(series, step = 2.0)

# Third method: bin bounds
bin_series(series, bounds = c(3,7))
```

## Black-Boxing Time Series

### Black-Box {#BlackBox}

__Interface:__

__Examples:__

### Black-Box Parts {#BlackBoxParts}

__Interface:__

__Examples:__

### {#}

__Interface:__

__Examples:__

## Coalescing Time Series

__Interface:__

__Examples:__

## Encoding/Decoding States

__Interface:__

__Examples:__

## Partitioning Time Series

__Interface:__

__Examples:__

## Random Time Series

__Interface:__

__Examples:__

## Time Series to TPM

__Interface:__

__Examples:__

# References {#References}