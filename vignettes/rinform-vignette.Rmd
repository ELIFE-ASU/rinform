---
title: "The rinform Wrapper"
author: "Gabriele Valentini"
date: "`r Sys.Date()`"
output:
#  rmarkdown::html_vignette
  html_document:
    toc: true # table of content true
    toc_depth: 2  # upto three depths of headings (specified by #, ## and ###)
    toc_float: true    
    number_sections: true  ## if you want number sections at each table header
    theme: yeti  # many options for theme, this one is my favorite.
    highlight: tango  # specifies the syntax highlighting style
  pdf_document:
    highlight: null
    number_sections: yes    
bibliography: vignette.bibtex
vignette: >
  %\VignetteIndexEntry{The rinform Wrapper}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

An R wrapper of the [Inform v1.0.0](https://elife-asu.github.io/Inform/) C library for performing information analysis of complex system. As for the Inform library, _rinform_ is structured around the concepts of:

* discrete emperical probability distributions which form the basis for
  all of the information-theoretic measures,
  
* classic information-theoretic measures built upon empirical distributions,

* measures of information dynamics for time series.

In addition to the core components, rinform also provides a small collection of utilities to deal with time series. 


# Getting Started

# Error Handling

# Empirical Distributions

## Distribution Type

## Allocation/Deallocation

## Accessors/Mutators

## Probabilities

# Shannon Information Measures

## Entropy

## Mutual Information

## Conditional Entropy

## Conditional Mutual Information

## Relative Entropy

## Cross Entropy

# Time Series Measures

The original purpose of [*Inform*](https://github.com/elife-asu/inform/) was to
analyze time series data. This explains why most of
*Inform*'s functionality resides in functions specifically optimized for analyzing time
series. Many information measures have "local" variants which compute a time series of point-wise
values. These feature is directly inherited by the *rinform* wrapper.

We have been meticulous in ensuring that function and parameter names are consistent
across measures. If you notice some inconsistency, please
[report it as an issue](https://github.com/elife-asu/rinform/issue).

## Notation

Throughout the discussion of time series measures, we will try to use a consistent notation.
We will denote random variables as $X,Y,\ldots$, and let $x_i,y_i,\ldots$
represent the $i$-th time step of a time series drawn from the associated random
variable. Many of the measures consider $k$-histories (a.k.a $k$-blocks) of the
time series, e.g.
$$x_i^{(k)} = \left\{x_{i-k+1}, x_{i-k+2},\ldots,x_i\right\}$$.

When denoting probability distributions, we will only make the random variable explicit in
situations where the notation is ambiguous. We will typically write $p(x_i)$,
$p(x_i^{(k)})$, and $p(x_i^{(k)}, x_{i+1})$ to denote the empirical probability
of observing the $x_i$ state, the $x_i^{(k)}$ $k$-history, and the joint
probability of observing $\left(x_i^{(k)}, x_{i+1}\right)$.

## Implementation Details

### The Base: States and Logarithms
The word "base" has two different meanings in the context of information measures on time
series. It could refer to the base of the time series itself, that is the number of unique
states in the time series. For example, the time series $\{0,2,1,0,0\}$ is a base-3
time series. On the other hand, it could refer to the base of the logarithm used in
computing the information content of the inferred probability distributions. The problem is
that these two meanings clash. The base of the time series affects the range of values the
measure can produce, and the base of the logarithm represents a rescaling of those values.

In the *rinform* wrapper we deal with this by *always* using base-2 logarithms and automatically
computing the base of each time series internally to the wrapper. All of this ensures that
the library is a simple as reasonably possible.

### Multiple Initial Conditions
You generally need *a lot* of data to infer a probability distribution.  An experimentalist
or simulator might then collect data over multiple trials or initial conditions. Most of
*rinform*'s time series measures allow the user to pass in a two-dimensional matrix
with each column representing a time series from a different initial condition. From
this the probability distributions are inferred, and the desired value is calculated. This
has the downside of requiring that the user store all of the data in memory, but it has the
advantage of being fast and simple.


## Active Information {#ActiveInformation}

Active information (AI) was introduced in [[@Lizier2012]](#References) to quantify information
storage in distributed computations. Active information is defined in terms of a
temporally local variant

$$
a_{X,i}(k) = \log_2{\frac{p(x_i^{(k)}, x_{i+1})}{p(x_i^{(k)})p(x_{i+1})}},
$$

where the probabilities are constructed empirically from the _entire_ time series.
From the local variant, the temporally global active information is defined as

$$
A_X(k) = \langle a_{X,i}(k) \rangle_i
       = \sum_{x_i^{(k)},x_{i+1}} p(x_i^{(k)},x_{i+1}) \log_2{\frac{p(x_i^{(k)}, x_{i+1})}{p(x_i^{(k)})p(x_{i+1})}}.
$$

Strictly speaking, the local and average active information are defined as

$$
a_{X,i} = \lim_{k\rightarrow \infty}a_{X,i}(k)
\qquad \textrm{and} \qquad
A_X = \lim_{k\rightarrow \infty}A_X(k),
$$

but we do not provide yet limiting functionality in this library
([GitHub issues](https://github.com/elife-asu/Inform/issues/24)).

__Interface:__

Compute the average or local active information with a history length `k`.

```r
active_info(series, k, local = FALSE)
```

__Examples:__

```{r}
# One initial condition:
series <- c(0, 0, 1, 1, 1, 1, 0, 0, 0)
active_info(series, k = 2)

# ..and local variant:
lai <- active_info(series, k = 2, local = T)
t(lai)

# Two initial conditions:
series      <- matrix(nrow = 9, ncol = 2)
series[, 1] <- c(0, 0, 1, 1, 1, 1, 0, 0, 0)
series[, 2] <- c(1, 0, 0, 1, 0, 0, 1, 0, 0)
active_info(series, k = 2)

# ..and local variant:
lai <- active_info(series, k = 2, local = T)
t(lai)

```

## Block Entropy {#BlockEntropy}

Block entropy, also known as $N$-gram entropy [[@Shannon1948]](#References),
is the standard Shannon entropy of the $k$-histories of a time series:
$$
H(X^{(k)}) = -\sum_{x_i^{(k)}} p(x_i^{(k)}) \log_2{p(x_i^{(k)})}
$$
which reduces to the traditional Shannon entropy for $k = 1$.

__Interface:__

Compute the average or local block entropy of a time series with block size `k`.

```r
block_entropy(series, k, local = FALSE)
```
__Examples:__

```{r}
# One initial condition:
# k = 1
series <- c(0, 0, 1, 1, 1, 1, 0, 0, 0)
block_entropy(series, k = 1)

# k = 2
block_entropy(series, k = 2)

# ..and local variant:
# k = 1
be <- block_entropy(series, k = 1, local = T)
t(be)

# k = 2
be <- block_entropy(series, k = 2, local = T)
t(be)

# Two initial conditions:
series      <- matrix(nrow = 9, ncol = 2)
series[, 1] <- c(0, 0, 1, 1, 1, 1, 0, 0, 0)
series[, 2] <- c(1, 0, 0, 1, 0, 0, 1, 0, 0)
block_entropy(series, k = 2)

# ..and local variant:
be <- block_entropy(series, k = 2, local = T)
t(be)
```

## Conditional Entropy

[Conditional entropy](https://en.wikipedia.org/wiki/Conditional_entropy) is a measure of the
amount of information required to describe a random variable $Y$ given knowledge of
another random variable $X$. When applied to time series, two time series are used to
construct the empirical distributions and the conditional entropy is given 
$$
H(Y|X) = - \sum_{x_i,y_i} p(x_i,y_i) \log_2{p(y_i|x_i)}.
$$

This can be viewed as the time-average of the local conditional entropy
$$
h_i(Y|X) = -\log_2{p(y_i|x_i)}.
$$
See [[@Cover1991]](#References) for more information.

__Interface:__

Compute the average and local conditional entropy between two time series.

This function expects the *condition* to be the first argument, `xs`. It is expected that
each time series be the same length `n`, but may have different bases.

```r
conditional_entropy(xs, ys, local = FALSE)
```

__Examples:__

```{r}
xs <- c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1)
ys <- c(0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1)

# Conditional entropy:
conditional_entropy(xs, ys)

conditional_entropy(ys, xs)

# ..and local variant:
ce <- conditional_entropy(xs, ys, local = T)
t(ce)

ce <- conditional_entropy(ys, xs, local = T)
t(ce)
```

## Cross Entropy

[Cross entropy](https://en.wikipedia.org/wiki/Cross_entropy) between two distributions
$p_X$ and $q_X$ measures the amount of information needed to identify events
using a coding scheme optimized for $q_X$ when $p_X$ is the "real" distribution
over $X$.
$$
H(p,q) = -\sum_{x} p(x) \log_2{q(x)}
$$
Cross entropy's local variant is equivalent to the self-information of $q_X$ and as
such is implemented by the [local block entropy](#BlockEntropy).

See [[@Cover1991]](#References) for more details.

__Interface:__

Compute the cross entropy between the "true" and "unnatural" distributions $p_X$ and
$q_X$ from associated time series `ps` and `qs`, respectively.

```r
cross_entropy(ps, qs)
```

__Examples:__

```{r}
ps <- c(0, 1, 1, 0, 1, 0, 0, 1, 0, 0)
qs <- c(0, 0, 0, 0, 0, 1, 1, 0, 0, 1)

cross_entropy(ps, qs)

cross_entropy(qs, ps)
```

## Effective Information

## Entropy Rate

[Entropy rate](https://en.wikipedia.org/wiki/Entropy_rate) quantifies the amount of
information needed to describe the next state of $X$ given observations of
$X^{(k)}.$  In other wrods, it is the entropy of the time series conditioned on the
$k$-histories.  The local entropy rate
$$
h_{X,i}(k) = \log_2{\frac{p(x_i^{(k)}, x_{i+1})}{p(x_i^{(k)})}}.
$$
can be averaged to obtain the global entropy rate
$$
H_X(k) = \langle h_{X,i}(k) \rangle_i
       = \sum_{x_i^{(k)},x_{i+1}} p(x_i^{(k)},x_{i+1}) \log_2{\frac{p(x_i^{(k)}, x_{i+1})}{p(x_i^{(k)})}}.
$$

Much as with [active information](#ActiveInformation), the local and average entropy rates are
formally obtained in the limit
$$
h_{X,i} = \lim_{k\rightarrow \infty}h_{X,i}(k)
\qquad \textrm{and} \qquad
H_X = \lim_{k\rightarrow \infty}H_X(k),
$$

but we do not provide yet limiting functionality in this library
([GitHub issues](https://github.com/elife-asu/Inform/issues/24)).

See [[@Cover1991]](#References) for more details.

__Interface:__

Compute the average or local entropy rate with a history length `k`.

```r
entropy_rate(series, k, local = FALSE)
```

__Examples:__

```{r}
# One initial condition:
series <- c(0, 0, 1, 1, 1, 1, 0, 0, 0)
entropy_rate(series, k = 2)

# ..and local variant:
er <- entropy_rate(series, k = 2, local = T)
t(er)

# Two initial conditions:
series      <- matrix(nrow = 9, ncol = 2)
series[, 1] <- c(0, 0, 1, 1, 1, 1, 0, 0, 0)
series[, 2] <- c(1, 0, 0, 1, 0, 0, 1, 0, 0)
entropy_rate(series, k = 2)

# ..and local variant:
er <- entropy_rate(series, k = 2, local = T)
t(er)
```

## Excess Entropy

## Information Flow

## Evidence of Integration

## Mutual Information

[Mutual information](https://en.wikipedia.org/wiki/Mutual_information) (MI) is a measure of
the amount of mutual dependence between at least two random variables. Locally, MI is
defined as
$$
i_i(X_1,\ldots,X_l) = \frac{p(x_{1,i},\ldots,x_{l,i})}{p(x_{1,i})\ldots p(x_{l,i})}.
$$
The mutual information is then just the time average of $i_i(X_1, \ldots, X_l)$:
$$
I(X_1,\ldots,X_l) =
    \sum_{x_{1,i},\ldots,x_{l,i}} p(x_{1,i},\ldots,x_{l,i}) \frac{p(x_{1,i},\ldots,x_{l,i})}{p(x_{1,i})\ldots p(x_{l,i})}.
$$

See [[@Cover1991]](#References) for more details.

__Interface:__

<<<<<<< HEAD
Compute the mutual information or its local variant between two or more time series.
Each variable can have a different base.

```r
mutual_info(series, local = FALSE)
```

__Examples:__

```{r}
# Two variables:
xs <- matrix(c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
               0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1), ncol = 2)

mutual_info(xs)

# ..and local variant:
mi <- mutual_info(xs, local = T)
t(mi)

# Three variables:
xs <- matrix(c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
               0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
               1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1), ncol = 3)

mutual_info(xs)

# ..and local variant:
mi <- mutual_info(xs, local = T)
t(mi)
```

## Partial Information Decomposition

## Predictive Information

## Relative Entropy

[Relative entropy](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence), also
known as the Kullback-Leibler divergence, measures the amount of information gained in
switching from a prior distribution $q_X$ to a posterior distribution $p_X$ over
_the same support_:
$$
D_{KL}(p||q) = \sum_{x_i} p(x_i) \log_2{\frac{p(x_i)}{q(x_i)}}.
$$
The local counterpart is
$$
d_{KL,i}(p||q) = log_2{\frac{p(x_i)}{q(x_i)}}.
$$
Note that the average in moving from the local to the non-local relative entropy is taken
over the posterior distribution.

See [[@Kullback1951]](#References) and [[@Cover1991]](#References) for more information.

__Interface:__

Compute the average and local relative entropy between time series drawn from posterior and prior
distributions, here `xs` and `ys` respectively.

```r
relative_entropy(xs, ys, local = FALSE)
```

__Examples:__

```{r}
xs <- c(0, 1, 0, 0, 0, 0, 0, 0, 0, 1)
ys <- c(0, 1, 1, 1, 1, 0, 0, 1, 0, 0)

# Average relative entropy:
relative_entropy(xs, ys)

relative_entropy(ys, xs)

# ..and local variant:
re <- relative_entropy(xs, ys, local = T)
t(re)

re <- relative_entropy(ys, xs, local = T)
t(re)
```

## Separable Information

## Transfer Entropy

# Utilities

## Binning Time Series

## Black-Boxing Time Series

## Coalescing Time Series

## Encoding/Decoding States

## Partitioning Time Series

## Random Time Series

## Time Series to TPM

# References {#References}